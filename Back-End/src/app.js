import express from 'express';
import cors from 'cors';
import helmet from 'helmet';
import morgan from 'morgan';
import dotenv from 'dotenv';
import axios from 'axios';
import multer from 'multer';
import { BlobServiceClient, generateBlobSASQueryParameters, BlobSASPermissions, SASProtocol, StorageSharedKeyCredential } from '@azure/storage-blob';
import { pool } from './db.js';

dotenv.config();
const app = express();
const upload = multer({ storage: multer.memoryStorage() });

// Trust reverse proxy (App Gateway / Azure Front Door / Nginx Ingress) when explicitly enabled
if ((process.env.TRUST_PROXY || '').toLowerCase() === '1' || (process.env.TRUST_PROXY || '').toLowerCase() === 'true') {
  app.set('trust proxy', true);
}

// --- Environment meta ---
const appEnv = process.env.APP_ENV || process.env.NODE_ENV || 'local';
// Accept new DEPLOY_TARGETS (comma list) or legacy DEPLOY_TARGET
const deployTargetsRaw = process.env.DEPLOY_TARGETS || process.env.DEPLOY_TARGET || 'webapp,vmss';
const deployTargets = deployTargetsRaw.split(',').map(v => v.trim()).filter(Boolean);
const isWebApp = deployTargets.includes('webapp');
const isVmss = deployTargets.includes('vmss');
const logLevel = (process.env.LOG_LEVEL || (appEnv === 'production' ? 'info' : 'debug')).toLowerCase();
const isProd = appEnv === 'production';


// --- Azure Document Intelligence ---
const diApiVersion = process.env.DI_API_VERSION || '2023-07-31';
async function analyzeReceiptByUrl(publicUrl, { modelIdEnv } = {}) {
  const endpoint = (process.env.AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT || '').replace(/\/$/, '');
  const apiKey = process.env.AZURE_DOCUMENT_INTELLIGENCE_KEY;
  const modelId = modelIdEnv || process.env.DI_MODEL_ID || 'prebuilt-receipt';
  if (!endpoint || !apiKey) {
    const err = new Error('Missing Document Intelligence configuration');
    err.status = 500; throw err;
  }
  const url = `${endpoint}/formrecognizer/documentModels/${modelId}:analyze?api-version=${diApiVersion}`;
  const body = { urlSource: publicUrl };
  let initial;
  try {
    initial = await axios.post(url, body, {
      headers: { 'Ocp-Apim-Subscription-Key': apiKey, 'Content-Type': 'application/json' },
      validateStatus: () => true
    });
  } catch (e) {
    e.status = e?.response?.status || 500; throw e;
  }
  if (initial.status !== 202 || !initial.headers['operation-location']) {
    const err = new Error('Analyze request not accepted');
    err.status = initial.status;
    err.details = initial.data; throw err;
  }
  const opLoc = initial.headers['operation-location'];
  let pollCount = 0; const maxPoll = 30; // ìµœëŒ€ 30ì´ˆ
  while (pollCount < maxPoll) {
    await new Promise(r => setTimeout(r, 1000));
    pollCount++;
    let pollRes;
    try {
      pollRes = await axios.get(opLoc, { headers: { 'Ocp-Apim-Subscription-Key': apiKey }, validateStatus: () => true });
    } catch (pe) {
      if (pollCount === maxPoll) { pe.status = 500; throw pe; }
      continue;
    }
    if (pollRes.status >= 400) {
      const err = new Error('Analyze poll error');
      err.status = pollRes.status; err.details = pollRes.data; throw err;
    }
    if (pollRes.data.status === 'succeeded') return pollRes.data;
    if (pollRes.data.status === 'failed') {
      const err = new Error('Analyze failed');
      err.status = 500; err.details = pollRes.data; throw err;
    }
  }
  const timeoutErr = new Error('Analyze timeout');
  timeoutErr.status = 504; throw timeoutErr;
}

// Environment and middleware baseline (updated to use APP_ENV / LOG_LEVEL)
const allowedOrigins = (process.env.CORS_ORIGINS || '')
  .split(',')
  .map(o => o.trim())
  .filter(Boolean);

app.use(cors({
  origin: (origin, cb) => {
    // allow server-to-server or same-origin without Origin header
    if (!origin) return cb(null, true);
    // allow all origins in non-prod for easier local dev
    if (!isProd) return cb(null, true);
    // in prod, restrict to configured list (empty list allows all)
    if (allowedOrigins.length === 0 || allowedOrigins.includes(origin)) return cb(null, true);
    return cb(new Error('CORS not allowed'));
  },
  credentials: true,
}));
app.use(helmet());
const morganFormat = logLevel === 'debug' ? 'dev' : 'combined';
if (logLevel !== 'silent') {
  app.use(morgan(morganFormat));
}
app.use(express.json({ limit: '2mb' }));
app.post('/api/upload-and-analyze', upload.single('image'), async (req, res) => {
  try {
    if (!req.file) {
      return res.status(400).json({ error: 'No file uploaded' });
    }
    // 1. Blob Storage ì—…ë¡œë“œ
    const accountName = process.env.AZURE_STORAGE_ACCOUNT_NAME;
    const accountKey = process.env.AZURE_STORAGE_ACCOUNT_KEY;
    const containerName = process.env.AZURE_BLOB_CONTAINER_NAME;
    const blobServiceClient = BlobServiceClient.fromConnectionString(
      `DefaultEndpointsProtocol=https;AccountName=${accountName};AccountKey=${accountKey};EndpointSuffix=core.windows.net`
    );
    const containerClient = blobServiceClient.getContainerClient(containerName);
    const blobName = Date.now() + '-' + req.file.originalname;
    const blockBlobClient = containerClient.getBlockBlobClient(blobName);
    await blockBlobClient.uploadData(req.file.buffer, {
      blobHTTPHeaders: { blobContentType: req.file.mimetype }
    });
    const imageUrl = `https://${accountName}.blob.core.windows.net/${containerName}/${blobName}`;

    // 2. Azure Document Intelligence ë¶„ì„
    // SAS (ì½ê¸°) 15ë¶„ ë¶€ì—¬ í›„ ê³µìš© URL êµ¬ì„±
    const sharedKeyCredential = new StorageSharedKeyCredential(accountName, accountKey);
    const sas = generateBlobSASQueryParameters({
      containerName,
      blobName,
      permissions: BlobSASPermissions.parse('r'),
      startsOn: new Date(),
      expiresOn: new Date(Date.now() + 15 * 60 * 1000)
    }, sharedKeyCredential).toString();
    const publicUrl = `https://${accountName}.blob.core.windows.net/${containerName}/${blobName}?${sas}`;
    if (process.env.NODE_ENV !== 'production') console.log('[DEBUG] Blob public SAS URL (temp):', publicUrl);

    let result;
    try {
      result = await analyzeReceiptByUrl(publicUrl, {});
    } catch (e) {
      const status = e.status || 500;
      if (process.env.NODE_ENV !== 'production') console.error('[DI ERROR]', status, e.details || e.message);
      return res.status(status).json({ error: 'AI analyze failed', details: e.details || e.message });
    }

    let receiptData = {};
    const docResult = result?.analyzeResult?.documents?.[0];
    if (docResult && docResult.fields) {
      const fields = docResult.fields;
      // valueString ìš°ì„ , ì—†ìœ¼ë©´ content ì‚¬ìš©
      const storeName = fields.MerchantName?.value || fields.MerchantName?.content || fields.store_name?.valueString || fields.store_name?.content || '';
      let totalAmountRaw = fields.Total?.value || fields.Total?.content || fields.total_amount?.valueString || fields.total_amount?.content || '';
      totalAmountRaw = String(totalAmountRaw).replace(/[^0-9.-]/g, '');
      const totalAmount = parseFloat(totalAmountRaw) || 0;
      let dateRaw = fields.TransactionDate?.value || fields.TransactionDate?.content || fields.transaction_date?.valueString || fields.transaction_date?.content || '';
      let transactionDate = '';
      if (dateRaw) {
        transactionDate = dateRaw.replace(/\./g, '-').replace(/-$/,'');
        if (/^\d{4}-\d{2}-\d{2}$/.test(transactionDate)) {
          transactionDate = transactionDate;
        } else {
          const parsedDate = new Date(dateRaw);
          if (!isNaN(parsedDate.getTime())) {
            transactionDate = parsedDate.toISOString();
          } else {
            transactionDate = '';
          }
        }
      }
      receiptData = {
        store_name: storeName,
        total_amount: totalAmount,
        transaction_date: transactionDate,
        source_blob_url: imageUrl
      };
    }
    res.json({ receipt: receiptData, imageUrl });
  } catch (err) {
    console.error('Upload & Analyze error:', err);
    res.status(500).json({
      error: 'Failed to upload or analyze image',
      details: err?.message || err,
      stack: err?.stack,
      raw: err
    });
  }
});


app.post('/api/upload-image', upload.single('image'), async (req, res) => {
  try {
    if (!req.file) {
      return res.status(400).json({ error: 'No file uploaded' });
    }
    const accountName = process.env.AZURE_STORAGE_ACCOUNT_NAME;
    const accountKey = process.env.AZURE_STORAGE_ACCOUNT_KEY;
    const containerName = process.env.AZURE_BLOB_CONTAINER_NAME;
    const blobServiceClient = BlobServiceClient.fromConnectionString(
      `DefaultEndpointsProtocol=https;AccountName=${accountName};AccountKey=${accountKey};EndpointSuffix=core.windows.net`
    );
    const containerClient = blobServiceClient.getContainerClient(containerName);
    const blobName = Date.now() + '-' + req.file.originalname;
    const blockBlobClient = containerClient.getBlockBlobClient(blobName);
    await blockBlobClient.uploadData(req.file.buffer, {
      blobHTTPHeaders: { blobContentType: req.file.mimetype }
    });
    const url = `https://${accountName}.blob.core.windows.net/${containerName}/${blobName}`;
    res.json({ url });
  } catch (err) {
    console.error('Image upload error:', err);
    res.status(500).json({ error: 'Image upload failed', details: err.message });
  }
});
// 1. ëª¨ë“  ëª¨ë“ˆ import í•˜ê¸°
// 1. ëª¨ë“  ëª¨ë“ˆ import í•˜ê¸°
// (Imports are now consolidated at the top)

// Import routers
// auth.jsì™€ receipts.jsì—ì„œ export default router;ë¥¼ ì‚¬ìš©í–ˆë‹¤ë©´ ì•„ëž˜ì²˜ëŸ¼ ì¤‘ê´„í˜¸ ì—†ì´ ê°€ì ¸ì™€ì•¼ í•©ë‹ˆë‹¤.
import { authRouter } from './auth.js';
import { receiptsRouter } from './receipts.js';
import { preferencesRouter } from './preferences.js';
import { analyticsRouter } from './analytics.js';


// 2. ì•± ìƒì„± ë° í™˜ê²½ë³€ìˆ˜ ë¡œë“œ



// 3. ë¯¸ë“¤ì›¨ì–´ ì„¤ì • (moved to top for clarity)


// 4. API ë¼ìš°íŠ¸ ì„¤ì •
// --- Health Check Endpoint (í†µí•©) ---
app.get('/health', (_, res) => {
  res.status(200).json({
    status: 'UP',
    time: new Date().toISOString(),
    commit: process.env.GIT_COMMIT || null,
    env: appEnv,
    deployTargets,
    isWebApp,
    isVmss,
    logLevel
  });
});

// --- DB Health Check Endpoint ---
app.get('/db-health', async (_, res) => {
  try {
    await pool.connect();
    res.status(200).json({ db: 'CONNECTED' });
  } catch (err) {
    res.status(500).json({ db: 'DISCONNECTED', error: err.message });
  }
});

// --- Azure Storage Health Check Endpoint ---
app.get('/storage-health', async (_, res) => {
  try {
    const blobServiceClient = BlobServiceClient.fromConnectionString(
      `DefaultEndpointsProtocol=https;AccountName=${process.env.AZURE_STORAGE_ACCOUNT_NAME};AccountKey=${process.env.AZURE_STORAGE_ACCOUNT_KEY};EndpointSuffix=core.windows.net`
    );
    const containerClient = blobServiceClient.getContainerClient(process.env.AZURE_BLOB_CONTAINER_NAME);
    const exists = await containerClient.exists();
    if (exists) {
      res.status(200).json({ storage: 'CONNECTED', container: 'FOUND' });
    } else {
      res.status(404).json({ storage: 'CONNECTED', container: 'NOT_FOUND' });
    }
  } catch (err) {
    res.status(500).json({ storage: 'DISCONNECTED', error: err.message });
  }
});

// --- Blob SAS í† í° ë°œê¸‰ API ---
app.get('/api/blob-sas', async (req, res) => {
  try {
    const { blobName } = req.query;
    if (!blobName) return res.status(400).json({ error: 'blobName is required' });

    const accountName = process.env.AZURE_STORAGE_ACCOUNT_NAME;
    const accountKey = process.env.AZURE_STORAGE_ACCOUNT_KEY;
    const containerName = process.env.AZURE_BLOB_CONTAINER_NAME;

    const sharedKeyCredential = new StorageSharedKeyCredential(accountName, accountKey);
    const expiresOn = new Date(Date.now() + 60 * 60 * 1000); // 1ì‹œê°„ í›„ ë§Œë£Œ
    const sasToken = generateBlobSASQueryParameters({
      containerName,
      blobName,
      permissions: BlobSASPermissions.parse('racw'), // ì½ê¸°, ì¶”ê°€, ìƒì„±, ì“°ê¸°
      startsOn: new Date(),
      expiresOn,
      protocol: SASProtocol.Https,
    }, sharedKeyCredential).toString();

    const sasUrl = `https://${accountName}.blob.core.windows.net/${containerName}/${blobName}?${sasToken}`;
    res.json({ sasUrl });
  } catch (err) {
    res.status(500).json({ error: err.message });
  }
});

// --- Azure Document Intelligence ì—°ê²° í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸ ---
app.get('/test-di', async (req, res) => {
  try {
    const testImageUrl = req.query.testImageUrl || 'https://aka.ms/azai/receipt-sample';
    try {
      const diResult = await analyzeReceiptByUrl(testImageUrl, { modelIdEnv: 'prebuilt-receipt' });
      res.status(200).json({ status: 'CONNECTED', data: isProd ? undefined : diResult });
    } catch (e) {
      res.status(e.status || 500).json({ status: 'DISCONNECTED', error: e.message, details: isProd ? undefined : e.details });
    }
  } catch (err) {
    res.status(500).json({ status: 'DISCONNECTED', error: err.message, details: isProd ? undefined : err.response?.data });
  }
});

// --- Azure Document Intelligence API ì—”ë“œí¬ì¸íŠ¸ ---
// ê¸°ì¡´ /api/document-intelligence ë¼ìš°íŠ¸ ì œê±° (upload-and-analyze ì‚¬ìš© í†µì¼)

// --- ë¶„ë¦¬ëœ API ë¼ìš°í„°ë“¤ ì—°ê²° ---
app.use('/api/auth', authRouter);
app.use('/api/receipts', receiptsRouter);
app.use('/api/preferences', preferencesRouter);
app.use('/api/analytics', analyticsRouter);

// (ì¤‘ë³µ ì œê±°ë¨)

// 5. ì„œë²„ ì‹¤í–‰ (PORT í™˜ê²½ë³€ìˆ˜ ìš°ì„ )
const port = process.env.PORT || 4000;
const server = app.listen(port, () => {
  console.log(`ðŸš€ Server is running on port ${port}`);
});

function shutdown(signal) {
  console.log(`\n${signal} received. Graceful shutdown start...`);
  server.close(err => {
    if (err) {
      console.error('Error during server close', err);
      process.exit(1);
    }
    console.log('HTTP server closed. Exiting.');
    process.exit(0);
  });
  // 10ì´ˆ ë‚´ ì¢…ë£Œ ê°•ì œ
  setTimeout(() => {
    console.warn('Force exit after timeout');
    process.exit(1);
  }, 10000).unref();
}
['SIGTERM','SIGINT'].forEach(sig => process.on(sig, () => shutdown(sig)));